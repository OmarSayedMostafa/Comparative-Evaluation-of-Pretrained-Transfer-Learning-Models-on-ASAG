{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Notes.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MLeYQq42GCS_","colab_type":"text"},"source":["# Transfer learning models\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jS8mxaGyyHb-","colab_type":"text"},"source":["* The features in the deep neural networks tend to change from general to task specific from start to the end. So the idea is to use the first layers and finetune or change the last layers for the different task. [source](https://arxiv.org/pdf/1801.06146.pdf)\n"]},{"cell_type":"markdown","metadata":{"id":"qC9lSTGFuNxd","colab_type":"text"},"source":["\n","## ELMo(Embeddings of Language Model)\n","<b>***Better for the disambiguation of different word senses***</b>\n","\n","* Deep contextualized words \n","* Trained on huge corpus of text data\n","* Elmo representations(aka embeddings) are the function of all the internal layers of biLM. Also represented that elmo representations are the function of the input sentence.\n","* <b>Architecture: </b> [Stacked LSTM layer](https://machinelearningmastery.com/stacked-long-short-term-memory-networks/)\n","* <b>Previous works:</b> \n","  - Assigning separate vectors for words in different meanings.\n","  - Context2vec : Encoding context around a pivot word by using biLSTM\n","  - BiRNNs: These also encode different information at different layers.\n","* Trained through semi-supervised learning approach.\n","\n","<b>Mathematics for bi language model:</b>\n","  \n","  * The joint probability distribution of a forward language model is\n","  \n","  $\\boxed {P(t_1,t_2,.......t_N) = \\Pi_{k=1}^N P(t_k|t_{k-1},t_{k-2},........t_1)} $\n","  \n","  * Similarly the joint distribution fro the backward language model is \n","  \n","  $\\boxed {P(t_1,t_2,.......t_N) = \\Pi_{k=1}^N P(t_k|t_{k+1},t_{k+2},........t_N)} $\n","  \n","  - This states that the backward language model takes the context of the word from the backwards.\n","  - A biLM combines the both forward and backward model and takes the maximum [log probability](https://en.wikipedia.org/wiki/Log_probability)\n","  \n","  $\\sum_{k=1}^N(log P(t_k|t_{k-1},t_{k-2},........t_1; \\theta_x, \\vec{\\theta_{LSTM}}, \\theta_s) + log P(t_k|t_{k+1},t_{k+2},........t_N; \\theta_x, \\overleftarrow{\\theta_{LSTM}}, \\theta_s))$\n","\n","<b>Mathematics for ELMo:</b>\n","  * Each token will have 2L+1 representations, where L is the number of layers of LSTM.\n","  \n","  $R_k =\\{\\vec{h_{k,j}^{LM}}, \\overleftarrow{h_{k,j}^{LM}}, x_k^{LM}\\}$\n","  \n","  * A token layer $h_{k,j}^{LM} = [\\vec{h_{k,j}^{LM}}, \\overleftarrow{h_{k,j}^{LM}}]$\n","  \n","  * ELMo converts all the layer representations of R into a single vector. In the simple case, it only considers the top most layer's representation  \"\"$h_{k,L}^{LM}$\"\"\n","  \n","  * Then compute a task specific weighting of biLM layers.\n","  \n","  $ELMO_k^{task} = E(R_k, \\theta^{task}) = \\gamma^{task}\\sum_{j=0}^L s_j^{task} h_{k,j}^{LM}$\n","  \n","    - where $s_j^{task}$ = normalized softmax weights\n","    - $\\gamma^{task}$ = scalar parameter to scale the representation vector\n","    \n","<b>Pretrained biLM architecture:</b>\n","* This architecture is to similar to the previous works done by [Jozefowicz et al,](https://arxiv.org/pdf/1602.02410.pdf) and [Kim et al.](https://arxiv.org/pdf/1508.06615.pdf). \n","* Additionally added residual connections between LSTM layers and modified for training on both sides.\n","* They have 2 layers of LSTM(L=2) with a residual connection from first to second\n","    \n","<b>Adding ELMo to supervised tasks:</b>\n","\n","* The biLM model can be improved better by adding ELMo to the supervised architecture for the desired NLP task.\n","* All the supervised NLP models have the similar architecture at the lowest layers.\n","* To add the ELMo to supervised models:\n","  - Freeze the biLM weights and add the ELMo vector $ELMo_K^{task}$ with $x_k$\n","  -  And pass this [$x_k$; $ELMo_K^{task}$] representation into the task RNN.\n","  \n","<b>What biLM representations are capturing?</b>\n","* As they are resulting better than word vectors, they are capturing more than what word vectors  are capturing.\n","* The nearby vectors of biLM represents the contextual words. An example is shown in the paper with word \"play\"\n","  \n","<b>Evaluated tasks:</b> \n","* Question-Answering\n","* Natural language inference(Textual entailment)\n","* Semantic role labelling\n","* Coreference resolution\n","* Named entity extraction\n","* Sentiment analysis\n","\n","<b>Keynotes:</b>\n","* Higher level LSTM represents the context of the word\n","* Lower level LSTM deals with the syntax of the word aspect.\n","* Not worked on semantic text similarity\n","* ELMo improved performance because of deep contextualized representations instead using only the top layers.\n","* Small regularization parameter(see \"Terms known\") is better for ELMo according to the authors.\n","* Including ELMo for the task specific biRNN architecture, at input and output results in the better results.\n","\n","<b>Terminology:</b>\n","* Regularization parameter ($\\lambda$): decreases the overfitting. See [here](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) and [here](https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression)\n","* Downstream models : ?\n","\n","\n","<b>  Questions: </b>\n","* What is coupled language model?\n","* What is function of language model?\n","* Why does in a stacked layers, the lower levels represent syntactic and upper levels the semantic features?\n","* What are character convolutions?\n","* What is task specific weighting?\n","* What are these supervised NLP models?? Are these Word2Vec and GloVe??\n","* The authors are talking about including ELMo for task related biRNNs. Do we use this BiRNNs architecture for every task?\n","\n","<b>Implementation:</b>\n","\n","* See under \"Using pretrained models\" in [For training the models as ELMo](https://github.com/allenai/bilm-tf)"]},{"cell_type":"markdown","metadata":{"id":"Fv7d20not8sC","colab_type":"text"},"source":["## BERT(Bidirectional Encoder Representations for Transformer)\n","\n","<b></b>\n","* Improving the finetuning approaches by bi directional encoder representations from Transformers.\n","* Introduced a new objective called as masked language model(\"MLM\") to test the effectiveness during training.\n","* Unlike ELMo approach of adding forward and backward representations, the authors used MLM to enable the pretrained deep bi directional embeddings.\n","* <b>Architecture :</b> [Multi layer Bidirectional Transformer Encoder](https://arxiv.org/pdf/1706.03762.pdf)\n","*<b> Previous approaches:</b>\n","  - Uni-directional pretraining\n","  - Finetuning pretraining models(BERT)\n","  - Feature based pretraining models(ELMo)\n","\n","\n","<b>Model architecture:</b>\n","* They stated that normally it is possible to train language models either left-to-right or right-to-left. We cannot do both at a time.\n","* Hence they have not gone to traditional language model training.\n","\n"," * <b> Masked LM: </b>\n","   - They masked some tokens and predicted the tokens during training the deep bidirectional representation.\n","   - Masked around 15% of random words in every sequence of WordPiece tokens and predicted only the masked words.\n","   - The disadvantage is that the words differ during finetuning and pretraining. So, 80% of the time they put the [mask] token, 10% of the time random word and 10% of the time correct word.\n","   - Eg: 80% of the time \"The dog has [mask]\", 10% of the time \"The dog has apple\", 10% of the time \"The dog has hair\". This induces bias to the correct word.\n","   - The training takes long time to converge as they were trained with only 15% of masked words at a time. However the results surpass the previous state of the arts.\n","   \n","  * <b> Next sentence prediction: </b>\n","   - A binarized next sentence prediction task was pretrained.\n","   - In pretraining 50% of the time correct sentences were shown and remaining 50% of the time some random sequence is given stating 'NotNext'\n","\n","<b>Finetuning procedures: </b>\n","  * For sentence classification task, the BERT approach is simple. \n","  * While finetuning the most of the hyperparameters are kept same. \n","  * Some hyperparameters such as \n","    - batchsize : 24-32\n","    - Learning rate : 5e-5, 3e-5, 2e-5\n","    - Epochs: 3-4 are said to be optimal for several tasks.\n","  * One of the key observation by the authors is that, the hyperparameters are very sensitive to larger datasets.\n","   \n","<b>Hyperparameters:</b>\n","\n","| Hyper parameter | Value/name |\n","| --- | --- |\n","| Batch size | 256 sequences/batch  (1,28,000 tokens per batch) |\n","| Epochs: 40 | 1,000,000 steps |\n","|  Optimizer | Adam |\n","| Learning rate | 1e-4 |\n","| Weight decay | 0.01|\n","| Momentum | $β_1 = 0.9, β_2 = 0.999.$ |\n","| Dropout probability | 0.1 |\n","| Activation function | gelu |\n","| Training loss | sum (mean masked LM likelihood, mean next sentence prediction likelihood) |\n","\n","<b>Evaluated tasks:</b> \n","\n","\n","| Tasks | Tested datasets | \n","| --- | --- |\n","| Natural Language Inference | MNLI <br> WNLI |\n","| Semantic Text Similarity | QQP <br> STS-B <br> MRPC |\n","| Question Answering | QNLI |\n","| Sentiment Analysis | SST-2 |\n","| Sentence Classification | CoLA |\n","| Binary Entailment | RTE |\n","\n","* All the tested datasets are GLUE benchmark datasets and results in GLUE score for each task.\n","* GLUE(General Language Understanding Evaluation) has the test data in the online corpus to test the model, for which labels are not provided.\n","* This evaluates the model and provide the GLUE score.\n","\n","<b>Keynotes:</b>\n","* Input embeddings of BERT are the sum of positional, segment and token embeddings.\n","* Good with sentence prediction, entailment and similairity tasks.\n","* The left-to-right language model performs worse in all the tasks compared to masked language modelling.\n","\n","<b>Terminology:</b>\n","* Feature based approach :  Uses the task specific architectures that include the pretraining representation as additional features.(Eg; ELMo)\n","* Fine tuning approach: Trained on downstream models by simple fine tuning(Eg: Open AI GPT, ULMFit)\n","\n","\n","<b>  Questions: </b>\n","* How does the masking helps?\n","* What is the difference between the masking language model and normal language model\n","* What is the difference between the steps and epochs?\n","\n","<b>Implementation:</b>\n","\n","* [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n"]},{"cell_type":"markdown","metadata":{"id":"slBvtzOGRfjZ","colab_type":"text"},"source":["## ULMFiT(Universal Language Model Fine Tuning)\n","\n","\"Pretraining the model on huge corpus and finetuning it on AWD LSTM architecture for target task\"\n","\n","\"On a note this model is primarily focused on classification tasks\"\n","\n","* The aim is to finetune the existing model and use it for various tasks in NLP.\n","* ULMFit had surpassed results with only 100 labelled examples.\n","* In addition to the model, they have also proposed 'discriminative fine-tuning', 'slanted triangular learning rates' and 'gradual unfreezing.'\n","* <b> Architecture:</b> [3- layered LSTM](https://arxiv.org/pdf/1708.02182.pdf)\n","\n","<b>Approach:</b>\n","* Assume two tasks source task ($\\tau_S$) and target task ($\\tau_T$).\n","* Language model is the ideal source task, which is used widely. The  authors had taken a state of the art language model trained on [AWD LSTM](https://arxiv.org/pdf/1708.02182.pdf)\n","* The model consists of following steps:\n","  - General model pretraining: Pretraining on Wikitext-103 dataset.Pretraining improves performance and converges the downstream tasks.\n","  - Target task LM finetuning: Finetuning the data on target task data. In finetuning here, they proposed \"descriminative finetuning and slanted triangular learning rates.\"  They are explained below.\n","* <b>Discriminative Finetuning:</b> This is the process of assigning different learning rates for different layers. This is because each layer learns a different level of feature(for eg: initial layers have high-level features and the final with low level features), so they are tuning different layer with different learning rate. The parameter ($\\theta$) at time 't' is given as follows:\n","\\begin{equation}\\theta_t = \\theta_{t-1} - \\eta \\nabla_{\\theta} J(\\theta)\\end{equation} \\begin{equation} \\implies \\theta_t^l = \\theta_{t-1}^l - \\eta^l \\nabla_{\\theta^l} J(\\theta)\\end{equation}\n","  - where $(\\theta_1, \\theta_2,....\\theta_l)$ and $(\\eta_1, \\eta_2,...... \\eta_l)$  are the parameters and learning rates of the layers 1,2,....l respectively.\n","  - $\\nabla_{\\theta}J(\\theta)$ represents the gradient of the model's objective function.\n","* <b>Slanted triangular learning rate (SLTR):</b> It is a proposed way of increasing the learning rates initially and later gradually decrease. This method is used for better convergence of parameters according to the task-specific feature.\n","* For finetuning the classifier, the authors attach the pretraining model to two layered linear blocks. Just as in computer vision, the two layers uses batch normalization, ReLU activation in between and softmax for the final layer.\n","* The parameters in these last two layers have been learnt from scratch.(Here we can sense the transfer learning)\n","* <b>Gradual unfreezing:</b> As the model forgets if we finetune all the layers at once, gradual unfreezing is introduced. In this process, intially the last layer is unfreezed and finetuned for one epoch, then the last two layers are taken and fineuned for another epoch and so on. Similarly for the total model is done.\n","* The preprocessing is carried out as same as [Mc.Cann et al](https://arxiv.org/pdf/1708.00107.pdf). In addition to that the authors added tokens for uppercase words, elongations and repetitions.\n","* Similar to ELMO, the authors pretrain both the forward and backward LM. Then they finetune the classifier for each LM using BPT3C(Backpropagation through time for text classification) and average them.\n","* Various analysis on pretraining, finetuning and bi-directionality is explained in the paper.\n","* The ULMFiT can be primarily used for\n","  - NLP for non-English languages, where training data is scarce.\n","  - NLP for new tasks, which donot have state of the art model.\n","  - Tasks with limited amount of labelled data.\n","\n","<b>Evaluated tasks:</b> \n","\n","* <b>Classification tasks:</b>\n","\n","| Tasks | Tested datasets | \n","| --- | --- |\n","| Sentiment analysis | IMDB dataset<br> Yelp review dataset |\n","| Question classification | small TREC dataset |\n","| Topic classification | AG news dataset <br> DBpedia ontology dataset |\n","\n","<b> Hyperparameters; </b>\n","\n","| Hyper parameter | Value/name |\n","| --- | --- |\n","|embedding size | 400 | \n","| Batch size | 64 for LM |\n","| Dropout | 0.4 to layers<br> 0.3 to RNN layers<br> 0.4 to input embedding layers<br> 0.05 to embedding layers<br> weight dropout of 0.5 to the RNN hidden-to-hidden matrix|\n","| Activation function | Adam |\n","| Momentum | $β_1$ = 0.7, $β_2$ = 0.99 |\n","| Finetuning learning rate | 0.004  for LM, 0.01for classifier|\n","| Training loss | sum (mean masked LM likelihood, mean next sentence prediction likelihood) |\n","\n","<b>keynotes:</b>\n","* Language model is the ideal source task used widely. This is because it can capture the \n","  - Context\n","  - Long-term dependencies\n","  - Heirarcheal relations and\n","  - Sentiment\n","* Language model provides a hypothesis space with all these features, which is useful for all the NLP tasks.\n","* Finetuning is said to be the most critical path  in transfer learning. \"Overly aggressive fine-tuning will cause catastrophic forgetting, eliminating the benefit of the information captured through language modeling; too cautious fine-tuning will lead to slow convergence (and resultant overfitting).\"\n","\n","<b>Terminology:</b>\n","* **Transductive inference:** Reasoning from observed specific cases while training to speicific test cases. \n"," - In [transductive transfer](https://www.andrewoarnold.com/arnolda-transfer-icdm-short.pdf) learning while training the test data is put into the training in which the both data are from same domain.\n","* **Inductive inference:** Reasoning from observed specific cases while training to general test cases. \n","* **Hypercolumns:** We get embeddings as features at different levels(Do you remember ELMo), then we use these in different ways. The concatenation of these embeddings at different layers is called as hypercolumn.\n","* **Multi-task learning:** Multiple tasks are solved at a time. For more see [here](https://en.wikipedia.org/wiki/Multi-task_learning)\n","* **Pooling:** As we obtain several features after convoluting/ finding embeddings with thousands of dimensions, we decrease the dimensions by aggregating the features. This process is called as pooling. This is done to decrease the computational cost. [Here](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) you can check the convolutional pooling and [here](http://jessicastringham.net/2018/12/30/conv-max-pool.html) the pooling in nlp.\n","* **Batch normalization:** It normalizes the output of previous layer by subtracting all the values from that layer's mean and dividing with layer's std deviation.\n","* **chain-thaw:** Sequentially unfreezes and finetunes single layer at a time. See [here](https://www.aclweb.org/anthology/D17-1169)\n","* **Supervised learning:** Labelled examples are used to finetune the LM.\n","* **Semi-supervised learning:** All task data is available and can be used to fine-tune the LM.\n","\n","<b>Questions</b>\n","* **What is convergence in machine learning/deep learning? Why is it required?**\n","  - Converging means as the iteration of algorithm goes on, the output value gets closer and closer.\n","* **What is multi-task learning? What is the difference between multi task learning and transfer learning?**\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"UtFnHLshE4_s","colab_type":"text"},"source":["## GPT (Generative Pre-Training)\n","\n","\"Generative pre-training of language model on unlabelled text data and supervised discriminative fine-tuning on each specific task\"\n","\n","* They use task-aware input transformation such that effective transfer during finetuning can be achieved, with minimal changes to model architecture.\n","* To attain more than word-level information is difficult for unlabelled text data for two main reasons:(acc to paper)\n","  - To know the better optimization to transfer best text representations.\n","  - The lack of single/general procedure to transfer representations.\n","* Considering these drawbacks, paper aims at creating an effective transfer learning through semi-supervised approach.\n","\n","<b>Approach:</b>\n","* The approach is the combination of unsupervised pre-training and supervised-finetuning.\n","   - First on an unlabelled data, language model objective is applied to learn initial parameters of the neural network\n","   - Then we finetune for the required task using corressponding supervised approach for the task derived from [traversal-style approaches](https://arxiv.org/pdf/1509.06664.pdf). This approach creates a single contiguous sequence of tokens for a structured text.\n","* *Transformer* architecture is used for the model, which has provided more structured memory for the long-term dependencies compared to recurrent neural networks.\n","\n","* <b>Unsupervised pre-training:</b>\n","\n"," * Given a corpus data with tokens $U = {u_1, u_2,.......u_n}$, we maximize the likelihood function:\n"," \n"," \\begin{equation} L_1(U) = \\sum_i log P(u_i|u_{i-1}, u_{i-2}.....u_{i-k}; \\theta) ------(1)\\end{equation}\n"," \n","   -  where $k$ is the size of the context window\n","   \n"," * They used multi-layer transformer(a variant of the transformer) decoder for the language model.\n","\n","  \\begin{equation}h_0 = UW_e + W_p \\end{equation}\n","  \\begin{equation}h_l = transformer\\_block(h_{l-1}) \\forall i \\in[1,n] -----(2)\\end{equation}\n","  \\begin{equation} P(u) = softmax(h_nW_e^T) \\end{equation}\n","\n","  - where $U = {u_1, u_2,.......u_n}$\n","  - $W_e$ = token embedding matrix\n","  - $W_p$ = Position embedding matrix\n","  - $h_0$ might be representing the input head of self-attention\n","  \n","* <b>Supervised fine-tuning :</b>\n","  - After training from first equation, we have to adapt the parameters to the target task.\n","  - For a labeled dataset $C$ assume sequence of tokens $x^1, x^2,..........,x^n$. The inputs are sent through the pretrained model to obtain the final transformer block's activation $h_l^m$, which is sent to the linear output layer with parameters $W_y$ to predict $y$:\n","\n","  - \\begin{equation}P(y|x^1,x^2,.....x^m) = softmax(h_l^m W_y)-----(3) \\end{equation}\n","\n","   -  Now we have to maximize the objective function\n","\n","\\begin{equation} L_2(C) = \\sum_{(x,y)} logP(y|x^1,.....x^m)------(4)\\end{equation}\n","\n","  -  One of the observation by the authors is that using language model as the auxiliary objective to fine-tune,\n","    - improved generalization of the supervised model\n","    - accelerating convergence\n","  -  Now we optimize the following objective, where $\\lambda$ is a weight :\n","\n","\\begin{equation} L_3(C) = L_2(C) + \\lambda L_1(C) \\end{equation}\n","\n","* <b>Task-specific input transformations:</b>\n"," -  Although it is easier to classify by this architecture, for the tasks like question answering, textual entailment need structured inputs like triplets of documents, questions and answers.\n"," - Hence we have to modify the model to apply to these tasks. For that the authors used a traversal style approach, in which the structured inputs are converted into an ordered sequence such that the pretrained model can process.\n"," - For similarity task, they have concatenated two texts by a delimiter in between in both the orders (text1 followed by text2 and viceversa) separately. Each order resulted in a representation $h_l^m$ and we add the both representations element-wise before feeding into the linear layer. See [figure](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n","\n","* They have trained the model with *BookCorpus* dataset.\n","\n","\n","<b>Evaluated tasks:</b>\n","\n","| Tasks | Tested datasets | \n","| --- | --- |\n","| Natural Language Inference | SNLI<br><br>MultiNLI<br><br>Question NLI<br><br> RTE<br><br>SciTail |\n","| Question Answering | RACE<br><br>Story Cloze |\n","| Semantic Similarity | MSR Paraphrase Corpus<br><br>Quora Question Pairs<br><br>STS Benchmark |\n","| Text classification | SST-2 <br><br> CoLA |\n","\n","<b>Hyperparameters:</b>\n","* **Pre-training:**\n","\n","| Hyperparameter | Vaue/Name |\n","| --- | --- |\n","| Attention heads | 12 |\n","| Dimensions | 768 |\n","| Optimizer | Adam |\n","| Max learning rate | 2.5e-4 |\n","| Epochs | 100 |\n","| Batchsize | 64 |\n","| Dropouts | 0.1 |\n","| Regularization | L2 |\n","| Activation function | Gaussian Error Linear Unit (GELU) |\n","\n","* **Finetuning:**\n","\n","| Hyperparameter | Vaue/Name |\n","| --- | --- |\n","| Dropout | 0.1 |\n","| Learning rate | 6.25e-5 | \n","|Batch size | 32 |\n","| Epochs | 3 |\n"," \n","\n","<b>Terminology:</b>\n","* **Discriminatively trained models:** The models which are trained for a specific task with corresponding datasets.\n","* **Sequence labelling:** A pattern recognition task which assigns each word with a categorical label. (Eg: Parts of speech tagging, Named Entity Recognition)\n","\n","<b>Preview:</b>\n","* The gpt tokenizer changes the word into root + suffix\n","* This tokenization is different from normal nltk tokenization. For eg:\n","  - In nltk tokenizer 'Abstraction' ---> \\[ 'Abstraction' ]\n","  - In gpt tokenizer 'Abstraction' ----> \\[ 'Abstract', 'tion' ] "]},{"cell_type":"markdown","metadata":{"id":"UTUd9MjVi2jx","colab_type":"text"},"source":["##GPT-2\n"," \"Our suspicion is that the prevalence of single task training\n","on single domain datasets is a major contributor to the lack\n","of generalization observed in current systems.\"\n","\n","  - The aim is to show that the language models can learn multi-tasks through unsupervised learning. The transfer learning can be done without any modifications of parameters and architectures. This also can attain the state of the art results.\n","\n","  - The largest GPT-2 model contains of 1.5B parameters, which achieves state of the art results in 7 out of 8 tasks.\n","\n","  - Although the various deep learning models provide greater accuracy, they are sensitive to the little changes in data distribution. These can provide good accuracy with only specific tasks and cannot function in generalized situations.\n","\n","  - Even the works like BERT and GPT projected that the task specific architectures are not required, instead the transfer of self-attention blocks does the work.\n","\n","  - So, the idea is to create more generalized and robust language model unsupervised, such that it can be used for various tasks without pretraining from scratch with corresponding datasets.\n","   \n","\n","<b>Approach:</b>\n","  - The previous approaches used combination of pretraining and supervised finetuning for the unsupervised language models which are more convenient for transfer learning.\n","  \n","  - Language modeling is the core part of the approach. Considering a general language model, we know that it is the unsupervised distribution of various examples $(x_1, x_2, ..., x_n)$ consisting of variable sequence length of symbols $(s_1,s_2,...,s_n).$ \n","\n","  \\begin{equation}\\implies P(x) = \\prod_{i=1}^n p(s_n|s_1,...........s_{n-1}) \\end{equation}\n","\n","  - Learning a single task can be considered as **P(output|input)**. Extending it too the generalized version, we can express that as **P(output|input, task)**.\n","\n","  - The byte-level language models are not effective on larger datasets such as One Billion Benchmark. \n","\n","  - Hence the authors used Byte Pair Encoding(BPE) which is effective for both the word-level and character-level language modeling.\n","\n","  -However the byte pair encoding often performs on Unicode code points resulting in the vocabulary of 1,30,000 symbols.\n","\n","  -Since this approach can effectively accredit the probability for the unicode strings, it can evaluate LM with any dataset irrespective of it's size, pre-processing etc.,\n","\n","  -<b>Architecture:</b>\n","    - They used the Attention mechanism and used the similar architecture of GPT with small changes.\n","    \n","    -The normalization layers are added to the input of each sub-block and also at the end of the final attention-block.\n","\n","  - They have trained four language models with log-uniformly spaced sizes.\n","\n","  - The smallest model is equivalent to the GPT model. The second smallest is equivalent to $BERT_{LARGE}$ model and the largest model is called as GPT-2.\n","\n","  - Initially, they have evaluated the zero-shot task transfer on language modeling.\n","\n","\n","<b>Dataset</b>\n","  - The dataset is chosen such that is more generalized without restricted to news articles, wikipedia or fiction books. \n","\n","  - They have chosen the web scrape (Common crawl) considering it to be more generalized data.\n","\n","  - However, this dataset has notable quality issues resulting in the authors creating a new qualitative webscrape from reddit, a social media platform.\n","\n","  -This new dataset created by the authors is called as WebText. \n","\n","  -They removed all the wikipedia data from the dataset for more precise comparative analysis with other approaches, which used that dataset.\n","\n","  - Although the LMs have not been pretrained or finetuned, they have resulted in state of the art results in various language models evaluating datasets because of the parameters and byte wise vocabulary.\n","\n","\n","<b>Hyperparameters</b>\n","\n","| Hyperparameter | Vaue/Name |\n","| --- | --- |\n","| Attention heads | 12 |\n","| Dimensions | 768 |\n","| Optimizer | Adam |\n","| Max learning rate | 2.5e-4 |\n","| Epochs | 100 |\n","| Batchsize | <b>512</b> |\n","| Dropouts | 0.1 |\n","| Regularization | L2 |\n","| Activation function | Gaussian Error Linear Unit (GELU) |\n","| Vocabulary size | 50,257 |\n","| Context size | 512-1024 |\n","\n","- The hyperparameters used for the four LMs are as follows:\n","\n","| Parameters |Layers | $d_{model}$ |\n","| --- | --- | --- |\n","| 117M | 12 | 768 | \n","| 345M | 24 | 1024 | \n","| 762M | 36 | 1280 |\n","| 1542M | 48 | 1600 |  \n","\n","<b>Evaluated tasks</b>\n","\n","| Tasks | Tested datasets |\n","| --- | --- |\n","| Language modeling | Children's Book Test (CBT) <br> LAMBADA <br> |\n","| Common sense reasoning | Winograd Schema Challenge |\n","| Reading comprehension | Conversation Question Answering Dataset (CQAD) |\n","| Summarization | CNN and Daily Mail dataset |\n","| Translation | WMT-14 English-French |\n","| Question Answering | Stanford Question Answering Dataset (SQUAD) |  \n","\n","<b>Key notes</b>\n","  - According to [McCann et al](https://arxiv.org/pdf/1806.08730.pdf), it is stated that the task, input and output can be represented as symbols for language to specify tasks. For eg: (translate from German to English, German text, English text).\n","  - Some of their experiments showed that the unsupervised learning is able to perform multi-tasks, but very slow to learn compared to exclusive supervised leaning.\n","  -Character level language modeling are very useful for infrequent words and word level for frequent words.\n","  - Most of the time the better-than-expected results occure due to the overlap of the train and test data. As the dataset size increases, this phenomena increases. This is one of the thing that can be happening with WebText acc., to the authors.\n","  \n","\n","<b>Terminology</b>\n","  * **Web scraping :** It is a process of collecting data from the web to a local database for retrieval or analysis.\n","  * **Byte Pair Encoding:**\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tar13KeO0EDn","colab_type":"text"},"source":["## Overview\n","\n","| Model | Architecture | Dataset name | Dataset size | Pretrained model\n","| --- | --- | --- | --- | --- |\n","| BERT | Transformer |BookCorpus <br> English Wikipedia | 800M words <br> 2500M words | [Installing bert embeddings](https://pypi.org/project/bert-embedding/) |\n","| ELMo | biLSTM | [One billion word benchmark](https://arxiv.org/pdf/1312.3005.pdf) | 1B words(30M sentences) | [Elmo embeddings](https://allennlp.org/elmo) |\n","| GPT | Transformer | BookCorpus | 800M words | [Installing GPT embeddings](https://github.com/huggingface/pytorch-transformers) | \n","| ULMFit | AWD LSTM | Wikitext-103 | 103M words | [ULMFit embeddings](http://files.fast.ai/models/wt103/) |\n"]},{"cell_type":"markdown","metadata":{"id":"5ZSqbnldLeBb","colab_type":"text"},"source":["# Finetuning from scratch using pretrained parameters\n"]},{"cell_type":"markdown","metadata":{"id":"3DuTZsOQRw0R","colab_type":"text"},"source":["## Creating vocab.txt file\n","\n","* As we donot need all the words of generalized area of domain, when working with the particular domain area of pretraining, we donot use the vocab.txt provided by BERT research.\n","* We create a new vocab.txt file from our corpus using this [code](https://github.com/kwonmha/bert-vocab-builder)."]},{"cell_type":"markdown","metadata":{"id":"mpoaolOxLiUD","colab_type":"text"},"source":["## BERT\n","###Source:\n"," [Pretraining BERT-Google Research](https://github.com/google-research/bert#pre-training-with-bert)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"floNaVH1s25s","colab_type":"text"},"source":["* Bert pretrained models consists of three files mainly:\n","  - **.ckpt:** Checkpoint file contains the pretrained weights.\n","  - **vocab.txt**: Vocabulary file mapping from wordpiece to word id.\n","  - **bert_config.json:** Hyperparameters of pretraining\n","* For pretraining the input file is a txt file.\n","* The documents are delimited by empty lines.\n","* For pretraining we have to run three files provided from google research.\n","  - create_pretraining.py\n","  - run_pretraining.py\n","  - **extract_features.py** (We have to do only this)\n"]},{"cell_type":"markdown","metadata":{"id":"7a4CWCJC1iJX","colab_type":"text"},"source":["* We can finetune the data on the BERT by using extract_features.py and model.ckpt.\n","* Here the training_data is our text books dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"uKCJrCcZNXVA","colab_type":"text"},"source":["## GPT\n"]},{"cell_type":"markdown","metadata":{"id":"tOIKnnSUOEat","colab_type":"text"},"source":["## ELMO\n","* The pretraining of elmo from scratch can be seen [here.](https://github.com/allenai/bilm-tf/blob/master/README.md#training-a-bilm-on-a-new-corpus)\n","\n","* The ELMo training works with GPU, but not with TPU."]},{"cell_type":"markdown","metadata":{"id":"Sja91BRaepz0","colab_type":"text"},"source":["# Sentence embeddings"]},{"cell_type":"markdown","metadata":{"id":"yF6UeinSes9G","colab_type":"text"},"source":["* Can be seen [here](https://towardsdatascience.com/fse-2b1ffa791cf9)."]},{"cell_type":"markdown","metadata":{"id":"aDhKF9_ffnXD","colab_type":"text"},"source":["# Semantic text similarity\n","   \n"]},{"cell_type":"markdown","metadata":{"id":"2cIrFjXAFfpl","colab_type":"text"},"source":["* Finding similarity between words is the fundamental process, which can be extended and implemented to setences, paragraphs and documents.\n","* Words can be similar \n","  - <b>Lexically:</b> If the words follow the similar character sequence. Lexical similarity can be checked by string based algorithms.\n","  - <b>Semantically: </b> If the words are meaningfully similar or used in the same context. Semantic similarity can be checked by corpus based and knowledge based algorithms.\n","\n","  \n","## Types\n","### String based:\n","* Measures the similarity from character compositions and string sequences. The string based algorithms are further divided into two types:\n","\n","#### Character based: \n","  * <b>Longest common substring:</b> It considers the contagious chain of characters in both the strings to measure the similarity.\n","  * <b>Damerau Levenshtein:</b>  The similarity is measured by counting the minimum number of operations required to transform one string to the other. Each operation can be defined as insertion, deletion, substitution or transposition of adjacent characters.\n","  * <b>Jaro:</b> It calculates similarity by considering the number and order of the common characters between strings.\n","  * <b>Jaro Winkler:</b> It is an extension of Jaro similarity, in which a prefix scale is given such that the favourable ratings are given for the characters matching in both the strings for that prefix length.\n","  * <b>Needleman Wunsch:</b> This algorithm performs a global alignment to find the best alignment for the entire of two sequences. This is a dynamic programming algorithm. This algorithm is efficient for the large sequence of similar texts.\n","  * <b>Smith-Waterman:</b> It is also a dynamic programming algorithm, in which local alignment is performed to find the best alignment of the entire of two sequences. This algorithm is efficient for the large sequence of dissimilar texts.\n","  * <b>N-grams:</b> In this the similarity is calculated by taking the n-grams of word/character sequences. Distance is calculated by taking the ratio of similar n-grams by maximal number of n-grams.\n","  \n","#### Term based:\n","  * <b>Block distance:</b> It is also known as Manhattan distance. It is calculated by the distance between two data points, when a grid-like path is followed. Mathematically, it is the sum of difference of corresponding components.\n","  * <b>Cosine similarity:</b> It is the cosine angle between two vectors representing the corresponding data points.\n","  * <b>Dice's coefficient:</b> It is the ratio of twice the common terms in two strings to the total number of terms in both the strings.\n","  * <b> Eucledian distance:</b> It is also called as L2 distance. It is the root of sum of squares of differences of corresponding components between the two vectors.\n","  * <b>Jaccard's distance:</b> It is the ratio of shared terms to the unique terms in both sentences.\n","  * <b>Matching coefficient:</b> It simply counts the number of similar items in both the vectors which are non-zero.\n","  * <b>Overlap coefficient:</b> It is similar to the Dice's coefficient, but considers the full match if one sentence is subset of the other. \n","\n","### Corpus based:\n","* The similarity between the words are measured from the information available from large corpora.\n","* <b>Hyperspace analogue to language (HAL):</b> \n","  - A matrix is created representing the semantic strength between the words using word co-occurences. The user can eliminate the low-entropy columns from the matrix.\n","  - The semantic strength is calculated by placing a center word in a window and accumulating the weights of cooccurences inversely proportional to the distance from the center word. It also considers whether the word has occured before or after the center word.\n","* <b>Latent semantic analysis (LSA):</b>\n","  - It assumes that the words occuring in similar context have similar semantic meaning.\n","  - A matrix is created with words as rows and paragraphs as columns. This matrix is reduced to lower number of columns by using the SVD technique. \n","  - Then cosine similarity is used to define the distance between two vectors formed by any two rows.\n","* <b>General latent semantic analysis (GLSA):</b>\n","  - It is the extension of the latent semantic analysis(LSA). GLSA focuses on the term vectors instead of the dual document representation.\n","  - Mostly it is similar to the latent semantic analysis, except changes in consideration of columns and cells.(NOT very sure)\n","* <b>Explicit semantic analysis (ESA):</b>\n","  - It can be used to measure the distance between two arbitrary texts. Each term in the text is assigned with tf-idf vector. \n","  - Then cosine similarity is used to find the distance between these two vectors.\n","* <b>Cross-language explicit semantic analysis (CESA):</b>\n","  - It is a multilanguage generalization of ECA. It considers the wikipedia to represent as the language independent vector.\n","  - The cosine distance of two corresponding vector representaitons are taken of two documents  in different languages to measure the relatedness.\n","  \n","  \n","\n","### Knowledge based:\n","* The similarity between words are measured from the information available from semantic networks.\n","\n","<b>Notes:</b>\n","* As our aim is to find the semantic similarity, string based is not much of our interest.\n","\n","<b>Questions:</b>\n","* What is contagious chain?\n"]},{"cell_type":"markdown","metadata":{"id":"gckXewk0gODn","colab_type":"text"},"source":["## Challenges\n"," * According to [GPT paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), the challenges of semantic text similarity task are:\n","   - Recognizing rephrasing of concepts\n","   - Understanding negation\n","   - Handling syntactic ambiguity"]},{"cell_type":"markdown","metadata":{"id":"E_b-nkZyGQs0","colab_type":"text"},"source":["## Applications"]},{"cell_type":"markdown","metadata":{"id":"ZfjGQlUjGTjx","colab_type":"text"},"source":["* Information retrieval\n","* Text classification\n","* Document clustering\n","* Topic detection\n","* Topic tracking\n","* Questions generation\n","* Question answering\n","* Essay scoring\n","* Short answer scoring\n","* Machine translation\n","* Text summarization"]},{"cell_type":"markdown","metadata":{"id":"y5pDVqkJ33CM","colab_type":"text"},"source":["## Metrics"]},{"cell_type":"markdown","metadata":{"id":"NdwiwiUW36HM","colab_type":"text"},"source":["* Root mean square error(RMSE): \n","  - RMSE is the mostly used metric to evaluate the model's performance of predicting the values.\n","  - It is the root of the quadratic mean of the difference of the values\n","  - Given $y_1, y_2, y_3,.....y_n$ as the actual values and $\\hat{y_1},\\hat{y_2},\\hat{y_3},.....\\hat{y_n}$ as the predicted values, the root mean square error can be defined as the \n","  \\begin{equation} RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2}\\end{equation}\n","  - Properties:    \n","    - The negative elements can lead the positive numbers to be zero.\n","    - To eliminate that, we square the differences and root it to scale it down.\n","    - The RMSE is proportional to the square of the error. So, a large error has larger effect on the metric.\n","    - Hence, the lower the metric, the better fit the model.\n","    - For these reasons, it is used as the standard metric in many tasks and also can be used to calculate accuracy.\n","    - However, this is not the best metric to evaluate the semantic text similarity model.\n","    \n","    \n","* Pearson correlation($\\rho$):\n","  - Although, RMSE is the standard metric for most of the tasks, it cannot be used as the appropriate metric for this specific task.\n","  - This is because, for a similarity task, when a similarity score is given it is subjective and changes from subject to subject.\n","  - So, it is important to follow the trend instead of the exact measure given. For this relation we have to consider the correlation between the scores instead of the differences.\n","  - Pearson introduced this metric to measure the direction of the scores. It is denoted by $\\rho$ and given by:\n","\n","  \\begin{equation} \\rho = \\frac{\\operatorname{cov}(y,\\hat{y})}{\\sigma_y \\sigma_{\\hat{y}}}\\end{equation}\n","  - where y and $\\hat{y}$ are the actual and predicted scores respectively.\n","  - Given the expectation of y as $\\operatorname{E}[y]$, the **cov** represents the covariance of the two variables and it is given by\n","  \\begin{equation} cov(y,\\hat{y}) = \\operatorname{E}{\\big[(y - \\operatorname{E}[y])(\\hat{y} - \\operatorname{E}[\\hat{y}])\\big]}\\end{equation}\n","  - and $\\sigma$ represents the standard deviation of the values of that particular variable.\n","   - The standard deviation is given by:\n","   \\begin{equation} \\sigma_y = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_i - \\bar{x})^2 }\\end{equation}\n","   - The range of pearson correlation($\\rho$) is from -1 to 1. The 1 states that it is linearly correlated. 0 states that it is linearly not correlated. and -1 states that it is negative linearly correlated.\n","   \n","\n","* Spearman correlation($r_s$):\n","  - however the pearson correlation defines linear correlation only.\n","  - To define the non-linear correlation, spearman came up with the extended version of pearson correlation, i.e., to calculate pearson correlation of the ranks of the variables.\n","  - This defines the non-linear correlation between the actual and predicted variables.\n","  - Hence, it is defined as the pearson correlation of the rank of the variables\n","  - The relation is given by:\n","    \\begin{equation} r_s =   = \\frac {\\operatorname{cov}(\\operatorname{rg}_y,\\operatorname{rg}_\\hat{y})} { \\sigma_{\\operatorname{rg}_y} \\sigma_{\\operatorname{rg}_\\hat{y}} } \\end{equation}\n","\n","    - where $rg_y$ is the rank variable of the variable y and $rg_\\hat{y}$ is the rank variable of variable $\\hat{y}$.\n","\n","  - For more detailed explanation of the metrics look in appendix\n","  -Create some plots for each metric\n","\n","\n","* Appendix:\n","    - For example, given a set of values \n","      - y = [0,1,2,3,4] and $\\hat{y}$ = [5,6,7,8,9]\n","    - \\begin{equation} RMSE(y, \\hat{y}) = \\sqrt{\\frac{1}{5}((0-5)^2 + (1-6)^2 + (2-6)^2 + (3-8)^2 + (4-9)^2)} \\end{equation}\n","   \\begin{equation}\\implies RMSE(y, \\hat{y}) = 5 \\end{equation}\n","  - whereas the pearson correlation $\\rho = 1$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pBR5YHz-NTHB","colab_type":"text"},"source":["# Various methods to calulate similarity\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wewI9WmDOln6","colab_type":"text"},"source":["## Cosine similarity\n"," * Create the embeddings of the words of the two sentences.\n"," * Create the mean vectors of the sentences.\n"," * Calculate the distance between the two mean vectors\n","\n"," <b>Notes:</b>\n"," * This method creates high weights to the irrelevant words.\n"," * The change in number of words in two sentences effect the accuracy of finding similarity. \n","  - For example: \"My health has been improved compared to the last year\" and \"I am healthier now\" are the two sentences with similar meaning, but the number of words are different, which can move the mean away."]},{"cell_type":"markdown","metadata":{"id":"RDtrOMmZR1UQ","colab_type":"text"},"source":["## Word mover's distance\n","* Remove the stop words in both the sentences.\n","* Each word in one sentence checks the nearer word in the second sentence.\n","* Then sum up the similarity and check which sentence is nearer.\n","\n","<b>Notes:</b>\n","* This is used to say which sentence is similar (out if many given sentences) to the given sentence.\n","* This cannot be used to say whether two sentences mean the same or not."]},{"cell_type":"markdown","metadata":{"id":"NQ5hey3dgPbd","colab_type":"text"},"source":["# Datasets"]},{"cell_type":"markdown","metadata":{"id":"xcCzpROGgRsM","colab_type":"text"},"source":["* Microsoft paraphrase corpus(MRPC)\n","* Quora question pairs(QQP)\n","* Semantic text similarity benchmark(STS-B)\n","* Mohler dataset\n"]},{"cell_type":"markdown","metadata":{"id":"4gBM-e_fTKoT","colab_type":"text"},"source":["# Results"]},{"cell_type":"markdown","metadata":{"id":"RWSJW1t9ORbM","colab_type":"text"},"source":["## Ridge regression \n","\n"]},{"cell_type":"markdown","metadata":{"id":"T17vx9k0Pc6g","colab_type":"text"},"source":["### RMSE\n","\n","| Model | Linear regression| Ridge regression | Ordinal ridge regression | \n","| --- | --- | --- | --- | \n","| ELMo | 0.965 | 0.965 | 1.010 |\n","| GPT | 1.060 | 1.060 | 1.103 | \n","| BERT | 1.051 | 1.052 | 1.102 | \n","| GPT-2 | 1.037 | 1.038 | 1.108 | "]},{"cell_type":"markdown","metadata":{"id":"ZKxDImv5PUBS","colab_type":"text"},"source":["### Pearson correlation\n","\n","| Model | Linear regression| Ridge regression | Ordinal ridge regression | \n","| --- | --- | --- | --- | \n","| ELMo | 0.465 | 0.440 | 0.379 |\n","| GPT | 0.247 | 0.175 | 0.008 | \n","| BERT | 0.287 | 0.170 | 0.105 | \n","| GPT-2 | 0.313 | 0.206 | 0.118 | "]},{"cell_type":"markdown","metadata":{"id":"37kKQXXNPgqM","colab_type":"text"},"source":["### Spearman correlation ($r_s$)\n","\n","\n","| Model | Linear regression| Ridge regression | Ordinal ridge regression | \n","| --- | --- | --- | --- | \n","| ELMo | 0.466 | 0.453 | 0.396 |\n","| GPT | 0.275 | 0.235 | -0.012 | \n","| BERT | 0.209 | 0.181 | 0.078 | \n","| GPT-2 | 0.180 | 0.168 | 0.065 | "]},{"cell_type":"markdown","metadata":{"id":"KGpOzEGSgbWJ","colab_type":"text"},"source":["## Semantic text similarity- Benchmarks"]},{"cell_type":"markdown","metadata":{"id":"yuR9kZKCTOt-","colab_type":"text"},"source":["| Model | STS-B | Mohler dataset |\n","| --- | --- | --- |\n","| ELMo | pearson = 0.65 [cite](https://arxiv.org/pdf/1806.06259.pdf) | --- |\n","| GPT | pearson = 0.823 | --- |\n","| BERT | corr = 0.8938784082825981<br>pearson = 0.8963020207577451<br> spearmanr = 0.8914547958074512 | --- |\n","| GPT-2 | --- | --- |\n"]},{"cell_type":"markdown","metadata":{"id":"MrsWA7NKOKSz","colab_type":"text"},"source":["# Planned Implementation approach\n","\n","## Training\n","* Train the dataset of computer science text books with transfer learning model architectures.\n","* Put in the expected answer and calculate the vector of the corresponding sentence\n","* Calculate the vector of the desired sentence\n","* Find the distance for every desired sentence and the input sentence.\n","* Train the model with sentence's distance and corresponding grade/target value with all the transfer learning model architectures.\n","\n","## Prediction\n","* Input the predicting sentence --> calculate the distance of this sentence vector with the expected sentence vector --> Predict the corresponding value"]},{"cell_type":"markdown","metadata":{"id":"-Tldz9cc_6Y5","colab_type":"text"},"source":["## Things to do in R&D\n","* Plot the graphs with binary/ trinary and 5 grades clusters\n","* Create correlation between the graders.\n","* Train BERT&ELMo "]},{"cell_type":"markdown","metadata":{"id":"sKYvGcWKAX4r","colab_type":"text"},"source":["# Questions and Answers\n","\n","<b>Date:</b> 19.06.2019\n","\n","* <b>Q. Explain sentence embeddings.</b>\n","* A. Sentence embeddings are analogus to word embeddings. In sentence embeddings sentences are mapped to vectors of real numbers. [source](https://en.wikipedia.org/wiki/Sentence_embedding)\n","\n","* <b>Q. Explain word embedding technique</b>\n","* A. Have to read the paper [(\"How to generate a good word embedding?\")](https://arxiv.org/pdf/1507.05523.pdf)\n","\n","* <b>Q. Explain Language representation model.</b>\n","* A. Language model is the probability distribution of words given a sequence. \n","\n","* <b>Q. What are word representations?</b>\n","* A. Word representations are same as word embeddings.\n","\n","* <b> Q. Difference between multi-purpose NLP models and embedding models.</b>\n","* A. ????\n"]},{"cell_type":"markdown","metadata":{"id":"8WAq0T4_7-_U","colab_type":"text"},"source":["# Resources"]},{"cell_type":"code","metadata":{"id":"pzPqXITfhDJG","colab_type":"code","outputId":"13fdd405-22cd-4ef5-83ff-7450fb78a347","executionInfo":{"status":"ok","timestamp":1565692078327,"user_tz":-120,"elapsed":1037,"user":{"displayName":"sasi kiran","photoUrl":"https://lh3.googleusercontent.com/-j5YQD1C_li8/AAAAAAAAAAI/AAAAAAAAAWI/hTaXjPh9svU/s64/photo.jpg","userId":"13357761986237341872"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["import tensorflow as tf\n","\n","for example in tf.python_io.tf_record_iterator(\"/content/sample_data/tf_examples.tfrecord\"):\n","    print(tf.train.Example.FromString(example))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Kn4kRYZMF9pr","colab_type":"text"},"source":["## Presentation details"]},{"cell_type":"markdown","metadata":{"id":"YaLakEtKvRxX","colab_type":"text"},"source":["* Less text and more images\n","* Introduce topic with image ------> what are we doing with image!!\n","* Talk more about results\n","* (Abstract + Results)---> important\n","* The abstract and results lead to abstract and introduction\n"]},{"cell_type":"code","metadata":{"id":"1oVB8LDs1wE8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}