{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ELMo_vocab.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FPjxiuNHOER2","colab_type":"text"},"source":["## Create ELMo Vocabulary file\n"]},{"cell_type":"markdown","metadata":{"id":"s4ZXaHW_e9Mh","colab_type":"text"},"source":["* Not required if you are doing finetuning\n"]},{"cell_type":"code","metadata":{"id":"uxwtSgX4FCky","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"62cf8b0b-cf14-4e3f-f46c-f2b43453374e","executionInfo":{"status":"ok","timestamp":1575307436518,"user_tz":-60,"elapsed":938,"user":{"displayName":"sasi kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBjLTu2dzjRCzp118mQDk2S6UyStqeosof2BUg3dQ=s64","userId":"13357761986237341872"}}},"source":["import nltk\n","nltk.download('punkt')\n","\n","from nltk import word_tokenize\n"],"execution_count":175,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MXlmezgtPtTC","colab_type":"code","colab":{}},"source":["def word_count_dict(PATH):\n","\n","  ifile = open(PATH,'r')\n","  lines = ifile.readlines()\n","\n","  wordlist = []\n","  countlist = []\n","\n","  for line in lines:\n","      # Get all the words in the current line\n","      words = word_tokenize(line)\n","\n","      words = line.split()\n","      for word in words:\n","          # Perform whatever manipulation to the word here\n","          # Remove any punctuation from the word\n","          # word = word.strip(\".,!?=:();'\\\"\")\n","          # Make the word lowercase\n","          word = word.lower()\n","\n","          # Add the word into wordlist only if it is not in wordlist\n","          if word not in wordlist:\n","            wordlist.append(word)\n","\n","          # Add the word to countlist so that it can be counted later\n","          countlist.append(word)\n","\n","  # Sort the wordlist\n","  wordlist.sort()\n","  count_dict = dict()\n","\n","  # Print the wordlist\n","  for word in wordlist:\n","      count_dict[word] = countlist.count(word)\n","    \n","  return count_dict\n","\n","word_count = word_count_dict('/content/sample_data/training_data.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8Z2ABiEO4nV","colab_type":"code","colab":{}},"source":["sorted_word_list = [word[0] for word in sorted(word_count.items(), key=lambda item: item[1])][::-1]\n","custom_tokens =['<S>', '</S>','<UNK>']\n","custom_tokens.extend(sorted_word_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lX8ERaZhOKFd","colab_type":"code","colab":{}},"source":["!>/content/elmo_vocab.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bs8TU6euNDka","colab_type":"code","colab":{}},"source":["with open('/content/sample_data/elmo_vocab.txt', 'w') as f:\n","    for item in custom_tokens:\n","        f.write(\"%s\\n\" % item)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7wYT61aaRRc9","colab_type":"text"},"source":["## Creating dataset"]},{"cell_type":"code","metadata":{"id":"2f-8IB3cUuPK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"55a4c8b7-fe91-41d8-9354-046ac0bbb84b","executionInfo":{"status":"ok","timestamp":1575307594935,"user_tz":-60,"elapsed":159295,"user":{"displayName":"sasi kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBjLTu2dzjRCzp118mQDk2S6UyStqeosof2BUg3dQ=s64","userId":"13357761986237341872"}}},"source":["!mkdir /content/training_dataset"],"execution_count":180,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/content/training_dataset’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MfFRA2f-RVMx","colab_type":"code","colab":{}},"source":["from random import shuffle\n","\n","def divide_chunks(lst, n): \n","  '''\n","  Divide the list into even chunks\n","  lst : list\n","  n: Number of required elements in each list\n","  '''      \n","    # looping till length l \n","  return [lst[i:i + n] for i in range(0, len(lst), n)]\n","\n","with open('/content/sample_data/training_data.txt','r') as txtfile:\n","  lines = txtfile.readlines()\n","sentences = [x.strip('.\\n') for x in lines] \n","\n","# Remove empty strings from the list\n","cleaned_sentences = (list(filter(None, content)))\n","\n","#Shuffle works in place\n","shuffle(cleaned_sentences)\n","\n","chunk_list = divide_chunks(cleaned_sentences, 1415)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3gWb8mTjXZX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"de798b2f-9029-47ee-a30f-1e638a137619","executionInfo":{"status":"ok","timestamp":1575307594945,"user_tz":-60,"elapsed":159288,"user":{"displayName":"sasi kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBjLTu2dzjRCzp118mQDk2S6UyStqeosof2BUg3dQ=s64","userId":"13357761986237341872"}}},"source":["print(len(chunk_list))"],"execution_count":182,"outputs":[{"output_type":"stream","text":["10\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M58uGCiOX_gr","colab_type":"text"},"source":["### Create text files in the folder"]},{"cell_type":"code","metadata":{"id":"FY-RVdS1WAkA","colab_type":"code","colab":{}},"source":["import os\n","\n","path = \"/content/training_dataset/\"\n","\n","for i in range(0,9):\n","\n","  file_path = path+\"data_\"+str(i)+'.txt'\n","  if not os.path.exists(file_path):\n","    os.mknod(file_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qdRVj2UfVYD_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2df90e51-dc99-4c22-a3f2-4de3bceb2b25","executionInfo":{"status":"ok","timestamp":1575307613952,"user_tz":-60,"elapsed":708,"user":{"displayName":"sasi kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBjLTu2dzjRCzp118mQDk2S6UyStqeosof2BUg3dQ=s64","userId":"13357761986237341872"}}},"source":["num = int(len(cleaned_sentences)/1415)\n","print(num)\n","for i in range(0,num):\n","  file_path = path+\"data_\"+str(i)+'.txt'\n","\n","  with open(file_path, 'w') as txtfile:\n","    for item in cleaned_sentences[:num]:\n","        txtfile.write(\"%s\\n\" % item)\n","  num = num+num\n"],"execution_count":185,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J9pNqBw5ROKI","colab_type":"text"},"source":["## Cloning elmo files"]},{"cell_type":"code","metadata":{"id":"QSd8B67XZsdW","colab_type":"code","colab":{}},"source":["# Downloading ELMo vocab\n","!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/vocab-2016-09-10.txt /content/sample_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpS-oDQUNxq6","colab_type":"code","colab":{}},"source":["! git clone https://github.com/allenai/bilm-tf /content/elmo_repo"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9h9IyqFmQ7rY","colab_type":"text"},"source":["### Finetuning data"]},{"cell_type":"code","metadata":{"id":"Nf8WVRmUaidX","colab_type":"code","colab":{}},"source":["!python /content/elmo_repo/bin/restart.py \\\n","        --save_dir /content/sample_data/checkpoint\\\n","        --vocab_file /content/vocab-2016-09-10.txt\\\n","        --train_prefix='/content/training_dataset/*'\\\n","        --n_epochs=5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXAl9agpbd2D","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}